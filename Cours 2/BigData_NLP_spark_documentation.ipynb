{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BigData - NLP spark documentation",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Notebook from Baptiste Guillemin & Paul Jaulin"
      ],
      "metadata": {
        "id": "JKYP1n3GQ_9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Start a Spark Session"
      ],
      "metadata": {
        "id": "y78HSD0AlqR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install -q pyspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('nlp').getOrCreate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQZ1FjpQloxG",
        "outputId": "13df85b5-8891-4572-e62f-ee542313e667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline : Estimator, Transformer, and Param"
      ],
      "metadata": {
        "id": "mN5nBhfVbD6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Pipeline is specified as a sequence of stages, and each stage is either a Transformer or an Estimator.\n",
        "\n",
        "•\tTransformer :\n",
        "method transform(), which converts one DataFrame into another, generally by appending one or more columns\t\n",
        "In the case of NLP transformer stages are basically converting:\n",
        "- Text to raw text ((fit() method). Add new column to dataframe.\n",
        "- Raw text to words (transform() method) \n",
        "- words into features vectors\n",
        "\n",
        "Then …\n",
        "•\tEstimators: are the model \n",
        "A Estimator implements a method fit(), which accepts a DataFrame and produces a Model, which is a Transformer. For example, a learning algorithm such as LogisticRegression is an Estimator, and calling fit() trains a LogisticRegressionModel, which is a Model and hence a Transformer.\n"
      ],
      "metadata": {
        "id": "Va5mvu84bW_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Prepare training data from a list of (label, features) tuples.\n",
        "training = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
        "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
        "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
        "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
        "\n",
        "# Create a LogisticRegression instance. This instance is an Estimator.\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
        "# Print out the parameters, documentation, and any default values.\n",
        "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
        "\n",
        "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
        "model1 = lr.fit(training)\n",
        "\n",
        "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
        "# we can view the parameters it used during fit().\n",
        "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
        "# LogisticRegression instance.\n",
        "print(\"Model 1 was fit using parameters: \")\n",
        "print(model1.extractParamMap())\n",
        "\n",
        "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
        "paramMap = {lr.maxIter: 20}\n",
        "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
        "# Specify multiple Params.\n",
        "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # type: ignore\n",
        "\n",
        "# You can combine paramMaps, which are python dictionaries.\n",
        "# Change output column name\n",
        "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # type: ignore\n",
        "paramMapCombined = paramMap.copy()\n",
        "paramMapCombined.update(paramMap2)  # type: ignore\n",
        "\n",
        "# Now learn a new model using the paramMapCombined parameters.\n",
        "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
        "model2 = lr.fit(training, paramMapCombined)\n",
        "print(\"Model 2 was fit using parameters: \")\n",
        "print(model2.extractParamMap())\n",
        "\n",
        "# Prepare test data\n",
        "test = spark.createDataFrame([\n",
        "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
        "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
        "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
        "\n",
        "# Make predictions on test data using the Transformer.transform() method.\n",
        "# LogisticRegression.transform will only use the 'features' column.\n",
        "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
        "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
        "prediction = model2.transform(test)\n",
        "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n",
        "    .collect()\n",
        "\n",
        "for row in result:\n",
        "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
        "          % (row.features, row.label, row.myProbability, row.prediction))"
      ],
      "metadata": {
        "id": "zgh4u7ZLbJ0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ],
      "metadata": {
        "id": "RHf6_PBVabgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TF-IDF feature vectorization approach is commonly used in text mining to express the value of a phrase or a term in a corpus. (Compile of documents).\n",
        "\n",
        "This a way to reduce the importance of too common words (like articles), and rely in the same time of the importance of the frequency of apparition within a corpus and the rarity in a single text.\n",
        "\n",
        "Mathematical expression:\n",
        "Wx,y = tf x,y x log (N/dfx)\n",
        "\n",
        "Where:\n",
        "\n",
        "Wx,y => Weight of word X within document Y\n",
        "AND\n",
        "\n",
        "tf x,y = frequency of word X in document Y\n",
        "\n",
        "d x = number of documents containing word X\n",
        "\n",
        "N = total number of documents\t"
      ],
      "metadata": {
        "id": "U9Fl81MYk7XD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZYVuqXIaG4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f09ba213-96c4-406c-e31e-ceda94dccc41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|label|features                                                                                                                                   |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0.0  |(20,[6,8,13,16],[0.28768207245178085,0.6931471805599453,0.28768207245178085,0.5753641449035617])                                           |\n",
            "|0.0  |(20,[0,2,7,13,15,16],[0.6931471805599453,0.6931471805599453,1.3862943611198906,0.28768207245178085,0.6931471805599453,0.28768207245178085])|\n",
            "|1.0  |(20,[3,4,6,11,19],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.6931471805599453,0.6931471805599453])                       |\n",
            "+-----+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0.0, \"Hi I heard about Spark\"),\n",
        "    (0.0, \"I wish Java could use case classes\"),\n",
        "    (1.0, \"Logistic regression models are neat\")\n",
        "], [\"label\", \"sentence\"])\n",
        "\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "rescaledData = idfModel.transform(featurizedData)\n",
        "\n",
        "rescaledData.select(\"label\", \"features\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##StopWordsRemover"
      ],
      "metadata": {
        "id": "UrChrzPcdN3M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are words which should be excluded from the input, typically because the words appear frequently and don’t carry as much meaning."
      ],
      "metadata": {
        "id": "25UOxObedh62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover\n",
        "\n",
        "sentenceData = spark.createDataFrame([\n",
        "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
        "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n",
        "], [\"id\", \"raw\"])\n",
        "\n",
        "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
        "remover.transform(sentenceData).show(truncate=False)"
      ],
      "metadata": {
        "id": "cyto_IZvdPUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##n -gram"
      ],
      "metadata": {
        "id": "h3XSt4mEagdv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-grams is a method to identify the value of sequence of words\n",
        "\n",
        "NGram takes as input a sequence of strings (the output of a Tokenizer)\n",
        "The output will consist of a sequence of nn-grams where each nn-gram is represented by a space-delimited string of n consecutive words."
      ],
      "metadata": {
        "id": "LZJl8MmtbsjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import NGram\n",
        "\n",
        "wordDataFrame = spark.createDataFrame([\n",
        "    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
        "    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
        "    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
        "\n",
        "ngramDataFrame = ngram.transform(wordDataFrame)\n",
        "ngramDataFrame.select(\"ngrams\").show(truncate=False)"
      ],
      "metadata": {
        "id": "VyfupMDPamrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec"
      ],
      "metadata": {
        "id": "S66tUQNaasrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The model maps each word to a unique fixed-size vector.\n",
        " This vector can then be used as features for prediction, document similarity calculations.\n",
        " \n",
        " The skip-gram model with softmax is expensive because the cost of computing logp(wi|wj) is proportional to V, which can be easily in order of millions. To speed up training of Word2Vec, we used hierarchical softmax, which reduced the complexity of computing of logp(wi|wj) to O(log(V))"
      ],
      "metadata": {
        "id": "iCY5Y79Cd6an"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Word2Vec\n",
        "\n",
        "# Input data: Each row is a bag of words from a sentence or document.\n",
        "documentDF = spark.createDataFrame([\n",
        "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
        "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
        "    (\"Logistic regression models are neat\".split(\" \"), )\n",
        "], [\"text\"])\n",
        "\n",
        "# Learn a mapping from words to Vectors.\n",
        "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
        "model = word2Vec.fit(documentDF)\n",
        "\n",
        "result = model.transform(documentDF)\n",
        "for row in result.collect():\n",
        "    text, vector = row\n",
        "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
      ],
      "metadata": {
        "id": "C0mjNXv-azaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example"
      ],
      "metadata": {
        "id": "nV7Pm4TQeZIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.mllib.feature import Word2Vec\n",
        "\n",
        "inp = sc.textFile(\"data/mllib/sample_lda_data.txt\").map(lambda row: row.split(\" \"))\n",
        "\n",
        "word2vec = Word2Vec()\n",
        "model = word2vec.fit(inp)\n",
        "\n",
        "synonyms = model.findSynonyms('1', 5)\n",
        "\n",
        "for word, cosine_distance in synonyms:\n",
        "    print(\"{}: {}\".format(word, cosine_distance))"
      ],
      "metadata": {
        "id": "i_gP3w9TeURE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CountVectorizer"
      ],
      "metadata": {
        "id": "GX18cwxjb3W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "# Input data: Each row is a bag of words with a ID.\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"a b c\".split(\" \")),\n",
        "    (1, \"a b b c a\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "\n",
        "# fit a CountVectorizerModel from the corpus.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
        "\n",
        "model = cv.fit(df)\n",
        "\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "id": "MBhDfrERb5C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LogisticRegression"
      ],
      "metadata": {
        "id": "0Maxqaxga30G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression, we **Predict a Binary Variable**, such as Yes/No, True/False, Passed/Not Passed, Pregnant/Not Pregnant, and so on, all of which can be coded 0 or 1, based on input values."
      ],
      "metadata": {
        "id": "sD1uopbcfKHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercept for logistic regression\n",
        "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
        "print(\"Intercept: \" + str(lrModel.intercept))\n",
        "\n",
        "# We can also use the multinomial family for binary classification\n",
        "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family=\"multinomial\")\n",
        "\n",
        "# Fit the model\n",
        "mlrModel = mlr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
        "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
        "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
      ],
      "metadata": {
        "id": "TRf2bdE2bU9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Naive Bayes"
      ],
      "metadata": {
        "id": "aUOBkgOhbeDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes is model of classification based on the bayes theoreme, which is conditional probabilities (oppose to frequency statistics)."
      ],
      "metadata": {
        "id": "m4bm8mCkct5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load training data\n",
        "data = spark.read.format(\"libsvm\") \\\n",
        "    .load(\"data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Split the data into train and test\n",
        "splits = data.randomSplit([0.6, 0.4], 1234)\n",
        "train = splits[0]\n",
        "test = splits[1]\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
        "\n",
        "# train the model\n",
        "model = nb.fit(train)\n",
        "\n",
        "# select example rows to display.\n",
        "predictions = model.transform(test)\n",
        "predictions.show()\n",
        "\n",
        "# compute accuracy on the test set\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
        "                                              metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test set accuracy = \" + str(accuracy))"
      ],
      "metadata": {
        "id": "dGJ0hIGqbiY1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}