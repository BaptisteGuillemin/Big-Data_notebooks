{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Exercise with Spark.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Student Name\n",
      ],
      "metadata": {
        "id": "kt4dQPvEVEHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zBGR8Bv8PMtp"
      },
      "outputs": [],
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "4EikmM9nPPdZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "5ApuuTo9RAl4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "wL7FKWh-RAjg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "lJIwWZ6wRAgs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qOiagWo9RAeL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark"
      ],
      "metadata": {
        "id": "A_GvJiBMPPbC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "hkiWD9sxPPYw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SMSSpamCollection').getOrCreate()"
      ],
      "metadata": {
        "id": "HOQHyHV-PPWo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(inferSchema='True',delimiter='\\t').csv(\"SMSSpamCollection.csv\")\n",
        "\n",
        "## Rename the columns\n",
        "df = df.withColumnRenamed(\"_c0\", \"class\").withColumnRenamed(\"_c1\", \"text\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ51pSpGz1f1",
        "outputId": "5eb63830-67dc-4c66-d60f-f9d170c51653"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- class: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oDZ9eUmB2WMH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a new length feature (new column w/ the length of the text)"
      ],
      "metadata": {
        "id": "1uwg52j74AF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "df = df.withColumn(\"textLength\", F.length(\"text\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDcWlmPK1F2U",
        "outputId": "b0079112-4f44-4165-9aa6-5d4196b08250"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+----------+\n",
            "|class|                text|textLength|\n",
            "+-----+--------------------+----------+\n",
            "|  ham|Go until jurong p...|       111|\n",
            "|  ham|Ok lar... Joking ...|        29|\n",
            "| spam|Free entry in 2 a...|       155|\n",
            "|  ham|U dun say so earl...|        49|\n",
            "|  ham|Nah I don't think...|        61|\n",
            "| spam|FreeMsg Hey there...|       147|\n",
            "|  ham|Even my brother i...|        77|\n",
            "|  ham|As per your reque...|       160|\n",
            "| spam|WINNER!! As a val...|       157|\n",
            "| spam|Had your mobile 1...|       154|\n",
            "|  ham|I'm gonna be home...|       109|\n",
            "| spam|SIX chances to wi...|       136|\n",
            "| spam|URGENT! You have ...|       155|\n",
            "|  ham|I've been searchi...|       196|\n",
            "|  ham|I HAVE A DATE ON ...|        35|\n",
            "| spam|XXXMobileMovieClu...|       149|\n",
            "|  ham|Oh k...i'm watchi...|        26|\n",
            "|  ham|Eh u remember how...|        81|\n",
            "|  ham|Fine if thats th...|        56|\n",
            "| spam|England v Macedon...|       155|\n",
            "+-----+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What do you notice ?"
      ],
      "metadata": {
        "id": "S4CzYynV3-0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visual inspection shows that the average length of the spam messages is longer."
      ],
      "metadata": {
        "id": "_Bxe_jf66n5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We can confirm this below\n",
        "df.groupby('class').mean().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVLpTb2m1Fy6",
        "outputId": "ab2333fa-8d93-49d6-fed5-0caf4d26c110"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|class|  avg(textLength)|\n",
            "+-----+-----------------+\n",
            "|  ham|71.45431945307645|\n",
            "| spam|138.6706827309237|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create feature transformers"
      ],
      "metadata": {
        "id": "SlKd5AzH4Q9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, NGram\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n"
      ],
      "metadata": {
        "id": "79OE9LdS1FnF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Pipeline to create a data pre-processing pipeline as follows "
      ],
      "metadata": {
        "id": "Ar5HB_F39P7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Use VectorAssembler to create an assembler of tf_idf feature with length \n",
        "## (column should be called ‘features’)\n",
        "\n",
        "## Change to True to use NGRAM\n",
        "use_ngram = False\n",
        "\n",
        "# Indexing class column to a numeric label (output column should be called ‘label’)\n",
        "data_to_num = StringIndexer(inputCol='class', outputCol='label')\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
        "\n",
        "# # • Ngrams (try with and without ngrams ; n=2)\n",
        "if use_ngram:\n",
        "  ngram = NGram(n=2, inputCol = \"token_text\", outputCol=\"ngrams\")\n",
        "  # • Count vertorization\n",
        "  count_vec = CountVectorizer(inputCol='ngrams', outputCol='count_vec_stop')\n",
        "\n",
        "  # • Stop words removal\n",
        "  # stop_remove = StopWordsRemover(inputCol='ngrams', outputCol='stop_tokens')\n",
        "else:\n",
        "  # • Stop words removal\n",
        "  stop_remove = StopWordsRemover(inputCol='token_text', outputCol='stop_tokens')\n",
        "  # • Count vertorization\n",
        "  count_vec = CountVectorizer(inputCol='stop_tokens', outputCol='count_vec_stop')\n",
        "\n",
        "\n",
        "# • IDF\n",
        "idf = IDF(inputCol=\"count_vec_stop\", outputCol=\"tf_idf\")\n",
        "\n",
        "# • Vector assembling\n",
        "vec_assembler = VectorAssembler(inputCols=['tf_idf', 'textLength'], outputCol='features')"
      ],
      "metadata": {
        "id": "7hka73y81FjV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LL9QNUMm1FLF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform the data DataFrame through the pipeline (last column should be called ‘features’)"
      ],
      "metadata": {
        "id": "Kru5OfBT90Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spam_pipe = Pipeline(stages=[data_to_num, tokenizer, ngram, stop_remove, count_vec, idf, vec_assembler])\n",
        "spam_cleaner = spam_pipe.fit(df)\n",
        "spam_data_clean = spam_cleaner.transform(df)"
      ],
      "metadata": {
        "id": "cxI7_ADy1E-2"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zzXCGdyy1E7f"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import NaiveBayse model from pyspark.ml.classification"
      ],
      "metadata": {
        "id": "wfwq4GM8AF19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes(featuresCol='features', labelCol='label')"
      ],
      "metadata": {
        "id": "wdeN6KRX1E3-"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a training DataFrame by selection the “label” and “features” column"
      ],
      "metadata": {
        "id": "NF_CyIbs1E1b"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_data_clean = spam_data_clean.select(['label','features'])\n",
        "spam_data_clean.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOlgwCUy1EwM",
        "outputId": "49d58588-2074-4866-851e-acc68a0dcb77"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(13424,[7,11,31,6...|\n",
            "|  0.0|(13424,[0,24,297,...|\n",
            "|  1.0|(13424,[2,13,19,3...|\n",
            "|  0.0|(13424,[0,70,80,1...|\n",
            "|  0.0|(13424,[36,134,31...|\n",
            "|  1.0|(13424,[10,60,139...|\n",
            "|  0.0|(13424,[10,53,103...|\n",
            "|  0.0|(13424,[125,184,4...|\n",
            "|  1.0|(13424,[1,47,118,...|\n",
            "|  1.0|(13424,[0,1,13,27...|\n",
            "|  0.0|(13424,[18,43,120...|\n",
            "|  1.0|(13424,[8,17,37,8...|\n",
            "|  1.0|(13424,[13,30,47,...|\n",
            "|  0.0|(13424,[39,96,217...|\n",
            "|  0.0|(13424,[552,1697,...|\n",
            "|  1.0|(13424,[30,109,11...|\n",
            "|  0.0|(13424,[82,214,47...|\n",
            "|  0.0|(13424,[0,2,49,13...|\n",
            "|  0.0|(13424,[0,74,105,...|\n",
            "|  1.0|(13424,[4,30,33,5...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Use random split to split the training data\n",
        "(train_data, test_data) = spam_data_clean.randomSplit([0.8, 0.2])\n",
        "\n",
        "## Train your model (fit method)\n",
        "predictor = nb.fit(train_data)\n"
      ],
      "metadata": {
        "id": "J1IKg5nGA2mB"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply your model to test data\n",
        "test_prediction = predictor.transform(test_data)\n",
        "test_prediction.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_XlgKKnA2jg",
        "outputId": "33e27e79-05d1-4fd7-c310-e765b3d5ac10"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(13424,[0,1,2,7,8...|[-795.25440005161...|[1.0,1.8518055687...|       0.0|\n",
            "|  0.0|(13424,[0,1,2,13,...|[-627.13652345556...|[0.99999999999720...|       0.0|\n",
            "|  0.0|(13424,[0,1,3,9,1...|[-573.79715989095...|[1.0,8.8327576374...|       0.0|\n",
            "|  0.0|(13424,[0,1,7,8,1...|[-882.32242070964...|[1.0,1.4921207925...|       0.0|\n",
            "|  0.0|(13424,[0,1,11,32...|[-873.03587670566...|[1.0,8.3423453660...|       0.0|\n",
            "|  0.0|(13424,[0,1,14,31...|[-216.45807846782...|[1.0,3.2073013608...|       0.0|\n",
            "|  0.0|(13424,[0,1,27,35...|[-1495.9605975636...|[0.99999899294740...|       0.0|\n",
            "|  0.0|(13424,[0,2,3,6,9...|[-3318.1758247734...|[1.0,7.6378616578...|       0.0|\n",
            "|  0.0|(13424,[0,2,3,8,2...|[-1611.5958242554...|[1.0,2.8342062931...|       0.0|\n",
            "|  0.0|(13424,[0,2,4,5,1...|[-2499.2685639078...|[1.0,1.1593669868...|       0.0|\n",
            "|  0.0|(13424,[0,2,4,40,...|[-1590.3456987618...|[0.99999977457356...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,8,1...|[-704.14435456552...|[1.0,1.6454242025...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,11,...|[-1334.1537616846...|[1.0,1.0011728667...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,27,...|[-486.46046423164...|[1.21135534001783...|       1.0|\n",
            "|  0.0|(13424,[0,2,7,31,...|[-672.63752911223...|[1.0,1.8563878175...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,114...|[-451.11353172515...|[1.0,5.3396026211...|       0.0|\n",
            "|  0.0|(13424,[0,2,8,28,...|[-1297.0698988825...|[1.0,6.9638897014...|       0.0|\n",
            "|  0.0|(13424,[0,2,10,13...|[-4859.4265629782...|[1.0,3.0120729060...|       0.0|\n",
            "|  0.0|(13424,[0,2,10,70...|[-842.64536540257...|[1.0,4.5851839231...|       0.0|\n",
            "|  0.0|(13424,[0,2,11,23...|[-1017.5489204580...|[1.0,8.5638398758...|       0.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eS7OuxNqA2gw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate accuracy using MulticlassClassificationEvaluator from pyspark.ml.evaluation"
      ],
      "metadata": {
        "id": "3GecoJ27CJn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "mcc_eval_acc = MulticlassClassificationEvaluator()\n",
        "model_acc = mcc_eval_acc.evaluate(test_prediction)\n",
        "\n",
        "print(\"Model Accuracy: {:.2f}\".format(model_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgbblN8CA2en",
        "outputId": "b6714713-297c-44e8-8ac6-c9c1072556b8"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p1d6PX4LA2bq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "#### Model Accuracy was 92% without NGrams\n",
        "#### Model Accuracy was 62% with NGrams\n"
      ],
      "metadata": {
        "id": "NTUg44OOFq_o"
      }
    }
  ]
}
